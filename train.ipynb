{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This program loads the training data created by `generate.ipynb` and creates a training model based on a semi-supervised approach toward common word endings. Lastly, it saves the training model as a list of rules using `pickle` such that `test.ipynb` can quickly load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"longest_substring\" is a utility method used to find the longest substring of s1 in s2 (assumption: a lemma can never be longer than its unmapped form, so s2 will always be a subset of s1)\n",
    "\n",
    "**note: `i==0` is an edge case because `s2[:-0] != s2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_substring(s1, s2):\n",
    "    for i in range(len(s2)):\n",
    "        if i==0:\n",
    "            if s1.find(s2) == 0:\n",
    "                return s2\n",
    "        if s1.find(s2[:-(i+1)]) == 0:\n",
    "            return s2[:-(i+1)]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lemma_mappings.txt\") as file:\n",
    "    items = file.readlines()\n",
    "items = [item.lstrip('\\t').rstrip(',\\n').split(':') for item in items if len(item) > 1]\n",
    "\n",
    "for item in items:\n",
    "    item[0] = item[0].replace('\\'','').lstrip('(').rstrip(')').split(', ')\n",
    "    item[1] = item[1].replace('\\'','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cell - make sure slicing is working as intended\n",
    "#min_end_len = 2\n",
    "#print(items[0][1])\n",
    "#print(items[0][1][-2:])\n",
    "#print()\n",
    "\n",
    "#for i in range(len(items[0][1])-(min_end_len-1)):\n",
    "#    print(items[0][1][-min_end_len-i:]+'/'+items[0][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"rules,\" which is a dict of lemma rules based on word endings of the training data set. The keys are given in the format [word_ending]/[POS_tag] and each corresponding value is a list containing [lemma mapping, count] where \"count\" is how many times that particular key maps to that bucket in the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {}\n",
    "min_end_len = 2\n",
    "for item in items:\n",
    "    if item[0][0] == item[1]:\n",
    "        if len(item[1]) < min_end_len:\n",
    "            continue\n",
    "        for i in range(len(item[1])-(min_end_len-1)):\n",
    "            key = item[1][-min_end_len-i:]+'/'+item[0][1]\n",
    "            if key not in rules:\n",
    "                rules[key] = [item[1][-min_end_len-i:], 1]\n",
    "            else:\n",
    "                rules[key][1] += 1\n",
    "    else:\n",
    "        if len(item[1]) < min_end_len:\n",
    "            continue\n",
    "        unmapped = item[0][0]\n",
    "        mapped = item[1]\n",
    "        diff = len(unmapped) - len(mapped)\n",
    "        ss = longest_substring(unmapped, mapped)\n",
    "        for i in range(len(ss)-(min_end_len-1)):\n",
    "            if i==0:\n",
    "                continue                                   # edge case - val would be full word\n",
    "            orig_ending = unmapped[-diff-i:]\n",
    "            key = unmapped[-diff-i:]+'/'+item[0][1]\n",
    "            val = mapped[-(len(orig_ending)-diff):]\n",
    "            if key not in rules:\n",
    "                rules[key] = [val, 1]\n",
    "            else:\n",
    "                rules[key][1] += 1\n",
    "        #print(longest_substring(unmapped, mapped))\n",
    "        #print(unmapped, mapped)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, sort the rules first by number of hits (e.g. give more weight to rules that are validated more often in the training set) and second by length of the key, as a tiebreaker (assuming, all else equal, a more specific/longer lemma match is more likely accurate than a shorter match, which could be more heavily influenced by non-relevant terms in the training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = sorted(rules.items(), key=lambda x:(x[1][1], len(x[0])), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and finally, write the generated model to `model.txt` for use in `test.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.txt', 'wb') as out_file:\n",
    "    pickle.dump(rules, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
